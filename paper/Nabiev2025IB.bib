@inproceedings{automl-zero,
  title={Automl-zero: Evolving machine learning algorithms from scratch},
  author={Real, Esteban and Liang, Chen and So, David and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={8007--8019},
  year={2020},
  organization={PMLR}
}
@inproceedings{Mitchell2007TheNF,
  title={The Need for Biases in Learning Generalizations},
  author={Tom Michael Mitchell},
  year={2007},
  url={https://api.semanticscholar.org/CorpusID:3237155}
}
@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
@INPROCEEDINGS{10803211,
  author={Kumar, S. Pradeep and Diwakar, Meenakshi and S, Jaishika and Arthi, P. and Pandi, V. Samuthira and D, Shobana},
  booktitle={2024 International Conference on Cybernation and Computation (CYBERCOM)}, 
  title={The Application of Machine Learning to Natural Language Processing: Modern Advances in the Study of Human Language}, 
  year={2024},
  volume={},
  number={},
  pages={561-566},
  keywords={Adaptation models;Visualization;Technological innovation;Translation;Computational modeling;Transfer learning;Machine learning;Predictive models;Natural language processing;Few shot learning;RNNs;LSTMs;Transformers;Pre-trained Language Models;BERT;GPT},
  doi={10.1109/CYBERCOM63683.2024.10803211}}
@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}
@InProceedings{ml_in_cv_survey,
author="Shekhar, Himanshu
and Seal, Sujoy
and Kedia, Saket
and Guha, Amartya",
editor="Mandal, Jyotsna Kumar
and Bhattacharya, Debika",
title="Survey on Applications of Machine Learning in the Field of Computer Vision",
booktitle="Emerging Technology in Modelling and Graphics",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="667--678",
abstract="Machine learning and computer vision have taken all over today's world which include medical diagnostics, statistical algorithm, logistic regression trees, etc. Such implementations are already been done in the arena such as smartphone applications, computer applications and online websites. We are getting a lot of features and advanced accessibility from machine learning and computer vision like Google Maps, Uber and Snapchat. We have put forth an introduction of some of the apps which use machine learning. We have also focussed on the algorithms used for machine learning implementation and their future uses. Some of the famous machine learning algorithms are random forest distribution algorithm, Na{\"i}ve Bayes algorithm, decision tree. In today's world of advanced technologies, cybersecurity has become paramount. Online threat analysis uses machine learning from the very grass-roots level. We have also discussed advancement done in medical science field through machine learning in computer vision.",
isbn="978-981-13-7403-6"
}

@article{OZBAYOGLU2020106384,
title = {Deep learning for financial applications : A survey},
journal = {Applied Soft Computing},
volume = {93},
pages = {106384},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106384},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620303240},
author = {Ahmet Murat Ozbayoglu and Mehmet Ugur Gudelek and Omer Berat Sezer},
keywords = {Deep learning, Finance, Computational intelligence, Machine learning, Financial applications, Algorithmic trading, Portfolio management, Risk assessment, Fraud detection},
abstract = {Computational intelligence in finance has been a very popular topic for both academia and financial industry in the last few decades. Numerous studies have been published resulting in various models. Meanwhile, within the Machine Learning (ML) field, Deep Learning (DL) started getting a lot of attention recently, mostly due to its outperformance over the classical models. Lots of different implementations of DL exist today, and the broad interest is continuing. Finance is one particular area where DL models started getting traction, however, the playfield is wide open, a lot of research opportunities still exist. In this paper, we tried to provide a state-of-the-art snapshot of the developed DL models for financial applications. We not only categorized the works according to their intended subfield in finance but also analyzed them based on their DL models. In addition, we also aimed at identifying possible future implementations and highlighted the pathway for the ongoing research within the field.}
}
@article{SHEHAB2022105458,
title = {Machine learning in medical applications: A review of state-of-the-art methods},
journal = {Computers in Biology and Medicine},
volume = {145},
pages = {105458},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105458},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522002505},
author = {Mohammad Shehab and Laith Abualigah and Qusai Shambour and Muhannad A. Abu-Hashem and Mohd Khaled Yousef Shambour and Ahmed Izzat Alsalibi and Amir H. Gandomi},
keywords = {Machine learning, Medical field, Diagnosis, Healthcare, Medical applications},
abstract = {Applications of machine learning (ML) methods have been used extensively to solve various complex challenges in recent years in various application areas, such as medical, financial, environmental, marketing, security, and industrial applications. ML methods are characterized by their ability to examine many data and discover exciting relationships, provide interpretation, and identify patterns. ML can help enhance the reliability, performance, predictability, and accuracy of diagnostic systems for many diseases. This survey provides a comprehensive review of the use of ML in the medical field highlighting standard technologies and how they affect medical diagnosis. Five major medical applications are deeply discussed, focusing on adapting the ML models to solve the problems in cancer, medical chemistry, brain, medical imaging, and wearable sensors. Finally, this survey provides valuable references and guidance for researchers, practitioners, and decision-makers framing future research and development directions.}
}
@article{autobert,
  author       = {Jiahui Gao and
                  Hang Xu and
                  Han Shi and
                  Xiaozhe Ren and
                  Philip L. H. Yu and
                  Xiaodan Liang and
                  Xin Jiang and
                  Zhenguo Li},
  title        = {AutoBERT-Zero: Evolving {BERT} Backbone from Scratch},
  journal      = {CoRR},
  volume       = {abs/2107.07445},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.07445},
  eprinttype    = {arXiv},
  eprint       = {2107.07445},
  timestamp    = {Wed, 01 Sep 2021 12:22:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-07445.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{automl4robots,
  title={Discovering adaptable symbolic algorithms from scratch},
  author={Kelly, Stephen and Park, Daniel S and Song, Xingyou and McIntire, Mitchell and Nashikkar, Pranav and Guha, Ritam and Banzhaf, Wolfgang and Deb, Kalyanmoy and Boddeti, Vishnu Naresh and Tan, Jie and others},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3889--3896},
  year={2023},
  organization={IEEE}
}
@article{Cranmer2023InterpretableML,
  title={Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl},
  author={M. Cranmer},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.01582},
  url={https://api.semanticscholar.org/CorpusID:258436785}
}
@INPROCEEDINGS{DLandInfoBottleneck,
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  keywords={Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training},
  doi={10.1109/ITW.2015.7133169}
}
@inproceedings{symb_ib_dl,
author = {Cranmer, Miles and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
title = {Discovering symbolic models from deep learning with inductive biases},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example—a detailed dark matter simulation—and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution-data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1462},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}
@article{bazarova_structure,
author = {Bazarova, A. I. and Grabovoy, A. V. and Strijov, V. V.},
title = {Analysis of the Properties of Probabilistic Models in Expert-Augmented Learning Problems},
year = {2022},
issue_date = {Oct 2022},
publisher = {Plenum Press},
address = {USA},
volume = {83},
number = {10},
issn = {0005-1179},
url = {https://doi.org/10.1134/S00051179220100058},
doi = {10.1134/S00051179220100058},
journal = {Autom. Remote Control},
month = oct,
pages = {1527–1537},
numpages = {11},
keywords = {interpretable model, linear model, expert-augmented learning, mixture of experts}
}
@book{kolmogorov1961representation,
title={On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables},
author={Kolmogorov, Andrey Nikolaevich},
year={1961},
  publisher={American Mathematical Society}
}
@article{RudStr13,
author = {Rudoy, G. I. and Strijov, V. V.},
title = {Algorithms for inductive generation of superpositions for approximation of experimental data},
year = {2013},
volume = {7},
number = {1},
journal = {Informatics and Applications},
pages = {44-53},
keywords = {symbolic regression, non-linear model, inductive generation, model complexity}
}
@inproceedings{multitask_ind_bias,
author = {Caruana, Rich},
title = {Multitask learning: a knowledge-based source of inductive bias},
year = {1993},
isbn = {1558603077},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Tenth International Conference on International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Amherst, MA, USA},
series = {ICML'93}
}
@article{He_2021,
   title={AutoML: A survey of the state-of-the-art},
   volume={212},
   ISSN={0950-7051},
   url={http://dx.doi.org/10.1016/j.knosys.2020.106622},
   DOI={10.1016/j.knosys.2020.106622},
   journal={Knowledge-Based Systems},
   publisher={Elsevier BV},
   author={He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
   year={2021},
   month=jan, pages={106622} }
@article{alex_net,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84–90},
numpages = {7}
}
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}
@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}
@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:13756489}
}
@article{baxter_ind_bias,
author = {Baxter, Jonathan},
title = {A model of inductive bias learning},
year = {2000},
issue_date = {February 2000},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {12},
number = {1},
issn = {1076-9757},
abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
journal = {J. Artif. Int. Res.},
month = mar,
pages = {149–198},
numpages = {50}
}
@article{survey_mtl,
  title={A Survey on Multi-Task Learning},
  author={Yu Zhang and Qiang Yang},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2017},
  volume={34},
  pages={5586-5609},
  url={https://api.semanticscholar.org/CorpusID:11311635}
}
@inproceedings{cosmo_ind_bias_sym_regression,
author = {Cranmer et al.},
title = {Discovering symbolic models from deep learning with inductive biases},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example—a detailed dark matter simulation—and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution-data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1462},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}
@article{Tishby2000TheIB,
  title={The information bottleneck method},
  author={Naftali Tishby and Fernando C Pereira and William Bialek},
  journal={ArXiv},
  year={2000},
  volume={physics/0004057},
  url={https://api.semanticscholar.org/CorpusID:8936496}
}
@article{ShwartzZiv2017OpeningTB,
  title={Opening the Black Box of Deep Neural Networks via Information},
  author={Ravid Shwartz-Ziv and Naftali Tishby},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.00810},
  url={https://api.semanticscholar.org/CorpusID:6788781}
}
@article{Alemi2017DeepVI,
  title={Deep Variational Information Bottleneck},
  author={Alexander A. Alemi and Ian Fischer and Joshua V. Dillon},
  journal={ArXiv},
  year={2017},
  volume={abs/1612.00410},
  url={https://api.semanticscholar.org/CorpusID:204922497}
}
@article{Qian2020MultiTaskVI,
  title={Multi-Task Variational Information Bottleneck},
  author={Weizhu Qian and Bowei Chen and Franck Gechter},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.00339},
  url={https://api.semanticscholar.org/CorpusID:220280268}
}
@inproceedings{Dorrell2022MetaLearningTI,
  title={Meta-Learning the Inductive Bias of Simple Neural Circuits},
  author={William Dorrell and Maria Yuffa and Peter E. Latham},
  booktitle={International Conference on Machine Learning},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:254018268}
}
@article{Liu2024KANKN,
  title={KAN: Kolmogorov-Arnold Networks},
  author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljacic and Thomas Y. Hou and Max Tegmark},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.19756},
  url={https://api.semanticscholar.org/CorpusID:269457619}
}
@article{KULUNCHAKOV2017221,
title = {Generation of simple structured information retrieval functions by genetic algorithm without stagnation},
journal = {Expert Systems with Applications},
volume = {85},
pages = {221-230},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417303354},
author = {A.S. Kulunchakov and V.V. Strijov},
keywords = {Information retrieval, Genetic programming, Ranking function, Evolutionary stagnation, Overfitting},
abstract = {This paper investigates an approach to construct new ranking models for Information Retrieval. The IR ranking model depends on the document description. It includes the term frequency and document frequency. The model ranks documents upon a user request. The quality of the model is defined by the difference between the documents, which experts assess as relative to the request, and the ranked ones. To boost the model quality a modified genetic algorithm was developed. It generates models as superpositions of primitive functions and selects the best according to the quality criterion. The main impact of the research if the new technique to avoid stagnation and to control structural complexity of the consequently generated models. To solve problems of stagnation and complexity, a new criterion of model selection was introduced. It uses structural metric and penalty functions, which are defined in space of generated superpositions. To show that the newly discovered models outperform the other state-of-the-art IR scoring models the authors perform a computational experiment on TREC datasets. It shows that the resulted algorithm is significantly faster than the exhaustive one. It constructs better ranking models according to the MAP criterion. The obtained models are much simpler than the models, which were constructed with alternative approaches. The proposed technique is significant for developing the information retrieval systems based on expert assessments of the query-document relevance.}
}
@article{Tishby2015DeepLA,
  title={Deep learning and the information bottleneck principle},
  author={Naftali Tishby and Noga Zaslavsky},
  journal={2015 IEEE Information Theory Workshop (ITW)},
  year={2015},
  pages={1-5},
  url={https://api.semanticscholar.org/CorpusID:5541663}
}
@misc{cohen2017inductivebiasdeepconvolutional,
      title={Inductive Bias of Deep Convolutional Networks through Pooling Geometry}, 
      author={Nadav Cohen and Amnon Shashua},
      year={2017},
      eprint={1605.06743},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1605.06743}, 
}
@article{DUBININ2024106179,
title = {Fading memory as inductive bias in residual recurrent networks},
journal = {Neural Networks},
volume = {173},
pages = {106179},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106179},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024001035},
author = {Igor Dubinin and Felix Effenberger},
keywords = {Recurrent neural network, Inductive bias, Residual connection, Memory},
abstract = {Residual connections have been proposed as an architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increased task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, those are residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks to capitalize on characteristic spectral properties of the data, and (iii) result in heterogeneous memory properties. In addition, we demonstrate how our results can be extended to non-linear residuals and introduce a weakly coupled residual initialization scheme that can be used for Elman RNNs.}
}
@misc{shwartzziv2017openingblackboxdeep,
      title={Opening the Black Box of Deep Neural Networks via Information}, 
      author={Ravid Shwartz-Ziv and Naftali Tishby},
      year={2017},
      eprint={1703.00810},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1703.00810}, 
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}
@ARTICLE{mnist,
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine}, 
  title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]}, 
  year={2012},
  volume={29},
  number={6},
  pages={141-142},
  keywords={Machine learning},
  doi={10.1109/MSP.2012.2211477}}
