@inproceedings{automl-zero,
  title={Automl-zero: Evolving machine learning algorithms from scratch},
  author={Real, Esteban and Liang, Chen and So, David and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={8007--8019},
  year={2020},
  organization={PMLR}
}
@article{autobert,
  author       = {Jiahui Gao and
                  Hang Xu and
                  Han Shi and
                  Xiaozhe Ren and
                  Philip L. H. Yu and
                  Xiaodan Liang and
                  Xin Jiang and
                  Zhenguo Li},
  title        = {AutoBERT-Zero: Evolving {BERT} Backbone from Scratch},
  journal      = {CoRR},
  volume       = {abs/2107.07445},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.07445},
  eprinttype    = {arXiv},
  eprint       = {2107.07445},
  timestamp    = {Wed, 01 Sep 2021 12:22:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-07445.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{automl4robots,
  title={Discovering adaptable symbolic algorithms from scratch},
  author={Kelly, Stephen and Park, Daniel S and Song, Xingyou and McIntire, Mitchell and Nashikkar, Pranav and Guha, Ritam and Banzhaf, Wolfgang and Deb, Kalyanmoy and Boddeti, Vishnu Naresh and Tan, Jie and others},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3889--3896},
  year={2023},
  organization={IEEE}
}
@article{Cranmer2023InterpretableML,
  title={Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl},
  author={M. Cranmer},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.01582},
  url={https://api.semanticscholar.org/CorpusID:258436785}
}
@INPROCEEDINGS{DLandInfoBottleneck,
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  keywords={Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training},
  doi={10.1109/ITW.2015.7133169}
}
@inproceedings{symb_ib_dl,
author = {Cranmer, Miles and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
title = {Discovering symbolic models from deep learning with inductive biases},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example—a detailed dark matter simulation—and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution-data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1462},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}
@article{bazarova_structure,
author = {Bazarova, A. I. and Grabovoy, A. V. and Strijov, V. V.},
title = {Analysis of the Properties of Probabilistic Models in Expert-Augmented Learning Problems},
year = {2022},
issue_date = {Oct 2022},
publisher = {Plenum Press},
address = {USA},
volume = {83},
number = {10},
issn = {0005-1179},
url = {https://doi.org/10.1134/S00051179220100058},
doi = {10.1134/S00051179220100058},
journal = {Autom. Remote Control},
month = oct,
pages = {1527–1537},
numpages = {11},
keywords = {interpretable model, linear model, expert-augmented learning, mixture of experts}
}
@book{kolmogorov1961representation,
title={On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables},
author={Kolmogorov, Andrey Nikolaevich},
year={1961},
  publisher={American Mathematical Society}
}
@article{RudStr13,
author = {Rudoy, G. I. and Strijov, V. V.},
title = {Algorithms for inductive generation of superpositions for approximation of experimental data},
year = {2013},
volume = {7},
number = {1},
journal = {Informatics and Applications},
pages = {44-53},
keywords = {symbolic regression, non-linear model, inductive generation, model complexity}
}